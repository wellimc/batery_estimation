{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077633c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e638d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['level']\n",
      "Training set shape == (69, 2)\n",
      "All timestamps == 69\n",
      "Featured selected: ['level']\n",
      "                  Date  level\n",
      "0  2022-03-28 18:02:00     68\n",
      "1  2022-03-28 18:03:00     68\n",
      "2  2022-03-28 18:04:00     68\n",
      "3  2022-03-28 18:05:00     68\n",
      "4  2022-03-28 18:06:00     68\n",
      "..                 ...    ...\n",
      "64 2022-03-28 19:06:00     60\n",
      "65 2022-03-28 19:07:00     60\n",
      "66 2022-03-28 19:08:00     60\n",
      "67 2022-03-28 19:09:00     60\n",
      "68 2022-03-28 19:10:00     59\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.read_csv('BatteryLog_360.csv', parse_dates=['Date'])\n",
    "\n",
    "#print(datelist)\n",
    "\n",
    "# Select features (columns) to be involved intro training and predictions\n",
    "cols = list(dataset_train)[1:2]\n",
    "print(cols)\n",
    "# Extract dates (will be used in visualization)\n",
    "# Extract dates (will be used in visualization)\n",
    "datelist_train = list(dataset_train['Date'])\n",
    "datelist_train = [date for date in datelist_train]\n",
    "\n",
    "print('Training set shape == {}'.format(dataset_train.shape))\n",
    "print('All timestamps == {}'.format(len(datelist_train)))\n",
    "print('Featured selected: {}'.format(cols))\n",
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a0b23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   level\n",
      "0   68.0\n",
      "1   68.0\n",
      "2   68.0\n",
      "3   68.0\n",
      "4   68.0\n",
      "..   ...\n",
      "64  60.0\n",
      "65  60.0\n",
      "66  60.0\n",
      "67  60.0\n",
      "68  59.0\n",
      "\n",
      "[69 rows x 1 columns]\n",
      "Shape of training set == (69, 1).\n",
      "[[680.]\n",
      " [680.]\n",
      " [680.]\n",
      " [680.]\n",
      " [680.]\n",
      " [680.]\n",
      " [680.]\n",
      " [680.]\n",
      " [670.]\n",
      " [670.]\n",
      " [670.]\n",
      " [670.]\n",
      " [670.]\n",
      " [670.]\n",
      " [660.]\n",
      " [660.]\n",
      " [660.]\n",
      " [660.]\n",
      " [660.]\n",
      " [660.]\n",
      " [660.]\n",
      " [650.]\n",
      " [650.]\n",
      " [650.]\n",
      " [650.]\n",
      " [650.]\n",
      " [650.]\n",
      " [650.]\n",
      " [640.]\n",
      " [640.]\n",
      " [640.]\n",
      " [640.]\n",
      " [640.]\n",
      " [640.]\n",
      " [640.]\n",
      " [630.]\n",
      " [630.]\n",
      " [630.]\n",
      " [630.]\n",
      " [630.]\n",
      " [630.]\n",
      " [630.]\n",
      " [630.]\n",
      " [620.]\n",
      " [620.]\n",
      " [620.]\n",
      " [620.]\n",
      " [620.]\n",
      " [620.]\n",
      " [620.]\n",
      " [610.]\n",
      " [610.]\n",
      " [610.]\n",
      " [610.]\n",
      " [610.]\n",
      " [610.]\n",
      " [610.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [600.]\n",
      " [590.]]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset_train[cols].astype(\"string\")\n",
    "print(dataset_train)\n",
    "for i in cols:\n",
    "    for j in range(0, len(dataset_train)):\n",
    "        dataset_train[i][j] = dataset_train[i][j].replace('.', '')\n",
    "\n",
    "dataset_train = dataset_train.astype(float)\n",
    "\n",
    "# Using multiple features (predictors)\n",
    "training_set = dataset_train.values\n",
    "\n",
    "print('Shape of training set == {}.'.format(training_set.shape)) \n",
    "training_set\n",
    "\n",
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf127fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.58229626]\n",
      " [ 1.58229626]\n",
      " [ 1.58229626]\n",
      " [ 1.58229626]\n",
      " [ 1.58229626]\n",
      " [ 1.58229626]\n",
      " [ 1.58229626]\n",
      " [ 1.58229626]\n",
      " [ 1.21469208]\n",
      " [ 1.21469208]\n",
      " [ 1.21469208]\n",
      " [ 1.21469208]\n",
      " [ 1.21469208]\n",
      " [ 1.21469208]\n",
      " [ 0.8470879 ]\n",
      " [ 0.8470879 ]\n",
      " [ 0.8470879 ]\n",
      " [ 0.8470879 ]\n",
      " [ 0.8470879 ]\n",
      " [ 0.8470879 ]\n",
      " [ 0.8470879 ]\n",
      " [ 0.47948371]\n",
      " [ 0.47948371]\n",
      " [ 0.47948371]\n",
      " [ 0.47948371]\n",
      " [ 0.47948371]\n",
      " [ 0.47948371]\n",
      " [ 0.47948371]\n",
      " [ 0.11187953]\n",
      " [ 0.11187953]\n",
      " [ 0.11187953]\n",
      " [ 0.11187953]\n",
      " [ 0.11187953]\n",
      " [ 0.11187953]\n",
      " [ 0.11187953]\n",
      " [-0.25572465]\n",
      " [-0.25572465]\n",
      " [-0.25572465]\n",
      " [-0.25572465]\n",
      " [-0.25572465]\n",
      " [-0.25572465]\n",
      " [-0.25572465]\n",
      " [-0.25572465]\n",
      " [-0.62332883]\n",
      " [-0.62332883]\n",
      " [-0.62332883]\n",
      " [-0.62332883]\n",
      " [-0.62332883]\n",
      " [-0.62332883]\n",
      " [-0.62332883]\n",
      " [-0.99093301]\n",
      " [-0.99093301]\n",
      " [-0.99093301]\n",
      " [-0.99093301]\n",
      " [-0.99093301]\n",
      " [-0.99093301]\n",
      " [-0.99093301]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.35853719]\n",
      " [-1.72614137]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.58229626],\n",
       "       [ 1.58229626],\n",
       "       [ 1.58229626],\n",
       "       [ 1.58229626],\n",
       "       [ 1.58229626],\n",
       "       [ 1.58229626],\n",
       "       [ 1.58229626],\n",
       "       [ 1.58229626],\n",
       "       [ 1.21469208],\n",
       "       [ 1.21469208],\n",
       "       [ 1.21469208],\n",
       "       [ 1.21469208],\n",
       "       [ 1.21469208],\n",
       "       [ 1.21469208],\n",
       "       [ 0.8470879 ],\n",
       "       [ 0.8470879 ],\n",
       "       [ 0.8470879 ],\n",
       "       [ 0.8470879 ],\n",
       "       [ 0.8470879 ],\n",
       "       [ 0.8470879 ],\n",
       "       [ 0.8470879 ],\n",
       "       [ 0.47948371],\n",
       "       [ 0.47948371],\n",
       "       [ 0.47948371],\n",
       "       [ 0.47948371],\n",
       "       [ 0.47948371],\n",
       "       [ 0.47948371],\n",
       "       [ 0.47948371],\n",
       "       [ 0.11187953],\n",
       "       [ 0.11187953],\n",
       "       [ 0.11187953],\n",
       "       [ 0.11187953],\n",
       "       [ 0.11187953],\n",
       "       [ 0.11187953],\n",
       "       [ 0.11187953],\n",
       "       [-0.25572465],\n",
       "       [-0.25572465],\n",
       "       [-0.25572465],\n",
       "       [-0.25572465],\n",
       "       [-0.25572465],\n",
       "       [-0.25572465],\n",
       "       [-0.25572465],\n",
       "       [-0.25572465],\n",
       "       [-0.62332883],\n",
       "       [-0.62332883],\n",
       "       [-0.62332883],\n",
       "       [-0.62332883],\n",
       "       [-0.62332883],\n",
       "       [-0.62332883],\n",
       "       [-0.62332883],\n",
       "       [-0.99093301],\n",
       "       [-0.99093301],\n",
       "       [-0.99093301],\n",
       "       [-0.99093301],\n",
       "       [-0.99093301],\n",
       "       [-0.99093301],\n",
       "       [-0.99093301],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.35853719],\n",
       "       [-1.72614137]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "print(training_set_scaled)\n",
    "sc_predict = StandardScaler()\n",
    "sc_predict.fit_transform(training_set[:, 0:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9e6bfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape == (6, 62, 0).\n",
      "y_train shape == (6, 1).\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 90 timestamps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "n_future = 2   # Number of days we want top predict into the future\n",
    "n_past = 62     # Number of past days we want to use to predict the future\n",
    "\n",
    "for i in range(n_past, len(training_set_scaled) - n_future +1):\n",
    "    X_train.append(training_set_scaled[i - n_past:i, 0:dataset_train.shape[1] - 1])\n",
    "    y_train.append(training_set_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "print('X_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c437784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and packages from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7615594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Neural Network based on LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Adding 1st LSTM layer\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(n_past, dataset_train.shape[1]-1)))\n",
    "\n",
    "# Adding 2nd LSTM layer\n",
    "model.add(LSTM(units=10, return_sequences=False))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "model.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d604936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321\n",
      "Epoch 1: val_loss improved from inf to 0.14534, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0321 - val_loss: 0.1453 - lr: 0.0025\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1482\n",
      "Epoch 2: val_loss improved from 0.14534 to 0.14186, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1482 - val_loss: 0.1419 - lr: 0.0025\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0769\n",
      "Epoch 3: val_loss improved from 0.14186 to 0.13416, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0769 - val_loss: 0.1342 - lr: 0.0025\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0749\n",
      "Epoch 4: val_loss improved from 0.13416 to 0.12544, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0749 - val_loss: 0.1254 - lr: 0.0025\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1284\n",
      "Epoch 5: val_loss improved from 0.12544 to 0.11312, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1284 - val_loss: 0.1131 - lr: 0.0025\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 6: val_loss improved from 0.11312 to 0.09819, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1087 - val_loss: 0.0982 - lr: 0.0025\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0451\n",
      "Epoch 7: val_loss improved from 0.09819 to 0.08754, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0451 - val_loss: 0.0875 - lr: 0.0025\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0419\n",
      "Epoch 8: val_loss improved from 0.08754 to 0.07725, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0419 - val_loss: 0.0773 - lr: 0.0025\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0849\n",
      "Epoch 9: val_loss improved from 0.07725 to 0.07465, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0849 - val_loss: 0.0746 - lr: 0.0025\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1280\n",
      "Epoch 10: val_loss improved from 0.07465 to 0.06838, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1280 - val_loss: 0.0684 - lr: 0.0025\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0398\n",
      "Epoch 11: val_loss improved from 0.06838 to 0.06171, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0398 - val_loss: 0.0617 - lr: 0.0025\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0564\n",
      "Epoch 12: val_loss improved from 0.06171 to 0.05892, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0564 - val_loss: 0.0589 - lr: 0.0025\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0518\n",
      "Epoch 13: val_loss improved from 0.05892 to 0.05526, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0518 - val_loss: 0.0553 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0460\n",
      "Epoch 14: val_loss improved from 0.05526 to 0.05246, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0460 - val_loss: 0.0525 - lr: 0.0025\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0897\n",
      "Epoch 15: val_loss improved from 0.05246 to 0.05152, saving model to weights.h5\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0897 - val_loss: 0.0515 - lr: 0.0025\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0777\n",
      "Epoch 16: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0777 - val_loss: 0.0522 - lr: 0.0025\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 17: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0368 - val_loss: 0.0536 - lr: 0.0025\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1542\n",
      "Epoch 18: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1542 - val_loss: 0.0543 - lr: 0.0025\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1745\n",
      "Epoch 19: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1745 - val_loss: 0.0557 - lr: 0.0025\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 20: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0984 - val_loss: 0.0570 - lr: 0.0025\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1474\n",
      "Epoch 21: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1474 - val_loss: 0.0622 - lr: 0.0025\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 22: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0108 - val_loss: 0.0671 - lr: 0.0025\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0630\n",
      "Epoch 23: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0630 - val_loss: 0.0748 - lr: 0.0025\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 24: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0972 - val_loss: 0.0840 - lr: 0.0025\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05152\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1004 - val_loss: 0.0950 - lr: 0.0025\n",
      "Epoch 25: early stopping\n",
      "CPU times: user 3.35 s, sys: 670 ms, total: 4.02 s\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(X_train, y_train, shuffle=True, epochs=30, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d01461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2022-03-28 19:10:00', freq='T'), Timestamp('2022-03-28 19:11:00', freq='T')]\n"
     ]
    }
   ],
   "source": [
    "# Generate list of sequence of days for predictions\n",
    "datelist_future = pd.date_range(datelist_train[-1], periods=n_future, freq='1min').tolist()\n",
    "print(datelist_future)\n",
    "'''\n",
    "Remeber, we have datelist_train from begining.\n",
    "'''\n",
    "\n",
    "# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE\n",
    "datelist_future_ = []\n",
    "for this_timestamp in datelist_future:\n",
    "    datelist_future_.append(this_timestamp.date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1408747",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform predictions\u001b[39;00m\n\u001b[1;32m      2\u001b[0m predictions_future \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train[\u001b[38;5;241m-\u001b[39mn_future:])\n\u001b[0;32m----> 4\u001b[0m predictions_train \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/keras/engine/training.py:1997\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1995\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(end_step, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m: batch_outputs})\n\u001b[1;32m   1996\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m batch_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected result of `predict_function` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1998\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(Empty batch_outputs). Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1999\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2000\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2001\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2002\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2003\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_end()\n\u001b[1;32m   2004\u001b[0m all_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure_up_to(batch_outputs, concat, outputs)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "# Perform predictions\n",
    "predictions_future = model.predict(X_train[-n_future:])\n",
    "\n",
    "predictions_train = model.predict(X_train[n_past:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54cf1d4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datetime\u001b[38;5;241m.\u001b[39mstrptime(x\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m y_pred_future \u001b[38;5;241m=\u001b[39m sc_predict\u001b[38;5;241m.\u001b[39minverse_transform(predictions_future)\n\u001b[0;32m---> 12\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m sc_predict\u001b[38;5;241m.\u001b[39minverse_transform(\u001b[43mpredictions_train\u001b[49m)\n\u001b[1;32m     14\u001b[0m PREDICTIONS_FUTURE \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y_pred_future, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mset_index(pd\u001b[38;5;241m.\u001b[39mSeries(datelist_future))\n\u001b[1;32m     15\u001b[0m PREDICTION_TRAIN \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y_pred_train, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mset_index(pd\u001b[38;5;241m.\u001b[39mSeries(datelist_train[\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n_past \u001b[38;5;241m+\u001b[39m n_future \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Inverse the predictions to original measurements\n",
    "\n",
    "# ---> Special function: convert <datetime.date> to <Timestamp>\n",
    "def datetime_to_timestamp(x):\n",
    "    '''\n",
    "        x : a given datetime value (datetime.date)\n",
    "    '''\n",
    "    return datetime.strptime(x.strftime('%d/%m/%Y %H:%M:%S'), '%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "\n",
    "y_pred_future = sc_predict.inverse_transform(predictions_future)\n",
    "y_pred_train = sc_predict.inverse_transform(predictions_train)\n",
    "\n",
    "PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=['level']).set_index(pd.Series(datelist_future))\n",
    "PREDICTION_TRAIN = pd.DataFrame(y_pred_train, columns=['level']).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))\n",
    "\n",
    "# Convert <datetime.date> to <Timestamp> for PREDCITION_TRAIN\n",
    "PREDICTION_TRAIN.index = PREDICTION_TRAIN.index.to_series().apply(datetime_to_timestamp)\n",
    "\n",
    "PREDICTION_TRAIN.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "predictions_future = model.predict(X_train[-n_future:])\n",
    "print(X_train[-n_future:])\n",
    "predictions_train = model.predict(X_train[n_past:])\n",
    "print(X_train[n_past:])\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Set plot size \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "\n",
    "# Plot parameters\n",
    "START_DATE_FOR_PLOTTING = ' 28/03/2022 18:02:00'\n",
    "\n",
    "plt.plot(PREDICTIONS_FUTURE.index, PREDICTIONS_FUTURE['level'], color='r', label='Predicted Stock Price')\n",
    "plt.plot(PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:].index, PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:]['level'], color='orange', label='Training predictions')\n",
    "plt.plot(dataset_train.loc[START_DATE_FOR_PLOTTING:].index, dataset_train.loc[START_DATE_FOR_PLOTTING:]['level'], color='b', label='Actual Stock Price')\n",
    "\n",
    "plt.axvline(x = min(PREDICTIONS_FUTURE.index), color='green', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.grid(which='major', color='#cccccc', alpha=0.5)\n",
    "\n",
    "plt.legend(shadow=True)\n",
    "plt.title('Predcitions and Acutal Stock Prices', family='Arial', fontsize=12)\n",
    "plt.xlabel('Timeline', family='Arial', fontsize=10)\n",
    "plt.ylabel('Stock Price Value', family='Arial', fontsize=10)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb94a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
